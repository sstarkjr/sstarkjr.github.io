---
title: 'Exploring PySpark: A Practice Session with Big Data'
date: 2025-01-04
permalink: /posts/2025/01/exploring-pyspark/
tags:
  - PySpark
  - Big Data
  - Distributed Computing
summary: 'A hands-on practice session with PySpark to understand its data processing capabilities on large datasets.'
---

# Exploring PySpark: A Practice Session with Big Data

---

Recently, I revisited some old PySpark practice notebooks to refresh my skills. While not groundbreaking, these notebooks are valuable resources for understanding PySpark and handling larger datasets.

This project focuses on exploring basic PySpark functionality with a large dataset, reinforcing distributed computing principles and demonstrating PySpark's practical applications.

---

## Why PySpark?

PySpark, Python’s interface to Apache Spark, efficiently handles large datasets, making it ideal for scenarios where tools like Pandas or traditional SQL struggle. Its distributed computing model and intuitive APIs make it a vital tool for data engineers and scientists. This session allowed me to explore its features and capabilities, providing insights into its potential for real-world challenges.

Unlike single-machine frameworks, PySpark scales effortlessly to process terabytes of data. Whether dealing with structured data or complex transformations, PySpark bridges the gap between raw data and actionable insights.

---

## The Dataset

The dataset was sourced from the [Amazon product reviews dataset](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews) (2023 version), curated by Julian McAuley and colleagues at UCSD. It contains customer reviews and metadata, making it ideal for testing big data tools. For this session, I focused on the `Musical Instruments` category, a structured subset perfect for exploration.

**Citation:**
```bibtex
@article{hou2024bridging,
  title={Bridging Language and Items for Retrieval and Recommendation},
  author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2403.03952},
  year={2024}
}
```

---

## Steps I Followed

### 1. Setting Up Spark

I created a Spark session as the starting point for interacting with PySpark.
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MusicalInstrumentsReviews") \
    .getOrCreate()
```

### 2. Loading the Data

I loaded the dataset using PySpark’s `read.json` function to explore its semi-structured format.
```python
file_path = "Musical_Instruments.jsonl"
df = spark.read.json(file_path)
df.printSchema()
```

### 3. Viewing Sample Rows

Displaying the first few rows verified that the data loaded correctly.
```python
df.show(5, truncate=False)
```

### 4. Exploring Basic Statistics

I calculated the total number of reviews and summary statistics for the `rating` column.
```python
print(f"Total reviews: {df.count()}")
df.select("rating").summary("mean", "min", "max").show()
```

### 5. Analyzing Rating Distribution

Grouping reviews by `rating` revealed customer sentiment trends.
```python
df.groupBy("rating").count().orderBy("rating").show()
```

### 6. Helpful Votes Analysis

I examined the `helpful_vote` column and calculated its correlation with `rating`.
```python
df.select("helpful_vote").summary("mean", "min", "max").show()
```

```plaintext
    +-------+------------------+
    |summary|      helpful_vote|
    +-------+------------------+
    |   mean|1.1371795751297706|
    |    min|                -1|
    |    max|              4650|
    +-------+------------------+
```
```python
df.corr("helpful_vote", "rating")
```
```plaintext
-0.014127523130376537
```

### 7. Verified Purchases Analysis

I analyzed the proportion of verified vs. unverified purchases.
```python
df.groupBy("verified_purchase").count().show()
```

### 8. Time-based Trends

Using the `timestamp` column, I explored time-based patterns by converting timestamps into human-readable dates.
```python
from pyspark.sql.functions import from_unixtime, to_date

df = df.withColumn("review_date", to_date(from_unixtime(df.timestamp / 1000)))
df.groupBy("review_date").count().orderBy("review_date").show()
```
```plaintext
    +-----------+-----+
    |review_date|count|
    +-----------+-----+
    | 1999-08-25|    1|
    | 1999-10-18|    1|
    | 2000-01-05|    1|
    | 2000-01-11|    1|
    | 2000-01-23|    1|
    | 2000-01-25|    1|
    | 2000-02-23|    1|
    | 2000-04-25|    1|
    | 2000-05-27|    1|
    | 2000-06-25|    1|
    | 2000-07-04|    1|
    | 2000-07-07|    1|
    | 2000-07-09|    1|
    | 2000-07-15|    1|
    | 2000-07-18|    1|
    | 2000-08-02|    1|
    | 2000-08-14|    1|
    | 2000-09-05|    1|
    | 2000-09-19|    1|
    | 2000-10-18|    1|
    +-----------+-----+

    +-----------+-----------+
    |review_date|avg(rating)|
    +-----------+-----------+
    | 1999-08-25|        5.0|
    | 1999-10-18|        1.0|
    | 2000-01-05|        5.0|
    | 2000-01-11|        4.0|
    | 2000-01-23|        5.0|
    | 2000-01-25|        4.0|
    | 2000-02-23|        1.0|
    | 2000-04-25|        4.0|
    | 2000-05-27|        5.0|
    | 2000-06-25|        5.0|
    | 2000-07-04|        1.0|
    | 2000-07-07|        5.0|
    | 2000-07-09|        5.0|
    | 2000-07-15|        5.0|
    | 2000-07-18|        2.0|
    | 2000-08-02|        4.0|
    | 2000-08-14|        4.0|
    | 2000-09-05|        5.0|
    | 2000-09-19|        3.0|
    | 2000-10-18|        4.0|
    +-----------+-----------+
```
### 10. Simple Word Count Text Analysis
Make sure that you have nltk install `pip install nltk`

```python
from pyspark.sql.functions import explode, split, lower, col
from nltk.corpus import stopwords

# stopwords: either use custom list or pre-loaded from nltk...
nltk_stop_words = set(stopwords.words("english"))

# Split review text into words
words_df = df.select(explode(split(lower(col("text")), "\\W+")).alias("word"))

# Remove stop words (if you have a list of stop words)
# stop_words = ["the", "and", "is", "to", "a", "i", "it", "of", "this", "in", "for", "with", "on", "was", "but"]

stop_words = [
    "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "aren't", "as", "at",
    "be", "because", "been", "before", "being", "below", "between", "both", "but", "by", "can't", "cannot", "could",
    "couldn't", "did", "didn't", "do", "does", "doesn't", "doing", "don't", "down", "during", "each", "few", "for",
    "from", "further", "had", "hadn't", "has", "hasn't", "have", "haven't", "having", "he", "he'd", "he'll", "he's",
    "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm",
    "i've", "if", "in", "into", "is", "isn't", "it", "it's", "its", "itself", "let's", "me", "more", "most", "mustn't",
    "my", "myself", "no", "nor", "not", "of", "off", "on", "once", "only", "or", "other", "ought", "our", "ours",
    "ourselves", "out", "over", "own", "same", "shan't", "she", "she'd", "she'll", "she's", "should", "shouldn't", "so",
    "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's",
    "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until",
    "up", "very", "was", "wasn't", "we", "we'd", "we'll", "we're", "we've", "were", "weren't", "what", "what's", "when",
    "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "won't", "would",
    "wouldn't", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves"
]

# filtered_words = words_df.filter(~col("word").isin(stop_words))
filtered_words = words_df.filter(~col("word").isin(nltk_stop_words))

# Most common words
filtered_words.groupBy("word").count().orderBy("count", ascending=False).show(20)

```

```plaintext
    +-------+-------+
    |   word|  count|
    +-------+-------+
    |       |2328596|
    |     br|1507946|
    |  great| 919159|
    |   good| 675913|
    |  sound| 609413|
    |    one| 566762|
    | guitar| 520538|
    |   like| 490100|
    |    use| 483565|
    |   well| 457718|
    |quality| 449197|
    |  would| 394134|
    |  price| 361456|
    |    get| 352493|
    |  works| 315143|
    |product| 314111|
    | really| 312490|
    |   nice| 301384|
    |   easy| 282064|
    |    mic| 262498|
    +-------+-------+
    only showing top 20 rows
```
## Product Analysis
```python
# Most reviewed products
df.groupBy("asin").count().orderBy("count", ascending=False).show(10)

# Products with the highest/lowest average ratings
df.groupBy("asin").avg("rating").orderBy("avg(rating)", ascending=False).show(10)
```
Returns
```plaintext
    +----------+-----+
    |      asin|count|
    +----------+-----+
    |B00DY1F2CS| 5047|
    |B01H74YV56| 4959|
    |B003LPTAYI| 4749|
    |B004XNK7AI| 4697|
    |B003VWJ2K8| 4131|
    |B018FCZKR2| 3911|
    |B0006NDF8A| 3799|
    |B00O4L3F9E| 3708|
    |B00063678K| 3547|
    |B002VA464S| 3521|
    +----------+-----+

    +----------+-----------+
    |      asin|avg(rating)|
    +----------+-----------+
    |B001FWYB9S|        5.0|
    |B007OCD89C|        5.0|
    |B005C9WDUO|        5.0|
    |B00ADQRRL6|        5.0|
    |B0016LXXV0|        5.0|
    |B01MRVHMGX|        5.0|
    |B000BK271Y|        5.0|
    |B07WR2PJWC|        5.0|
    |B00PNSDXWQ|        5.0|
    |B006YV2XVI|        5.0|
    +----------+-----------+

```

### 9. ASIN-Specific Trends

I analyzed review counts over time for a specific product (`ASIN: B00DY1F2CS`) at daily, weekly, and monthly intervals.

#### Daily Reviews
```python
asin_df = df.filter(df.asin == "B00DY1F2CS")
review_counts = asin_df.groupBy("review_date").count().orderBy("review_date", ascending=False)
review_counts.show()
```
![Daily Reviews](/images/output_23_1.png)

#### Weekly Reviews
```python
from pyspark.sql.functions import date_trunc

weekly_df = asin_df.withColumn("week_start", date_trunc("week", asin_df.review_date))
weekly_review_counts = weekly_df.groupBy("week_start").count().orderBy("week_start", ascending=False)
weekly_review_counts.show()
```
![Weekly Reviews](/images/output_26_1.png)

#### Monthly Reviews
```python
monthly_df = asin_df.withColumn("month_start", date_trunc("month", asin_df.review_date))
monthly_review_counts = monthly_df.groupBy("month_start").count().orderBy("month_start", ascending=False)
monthly_review_counts.show()
```
![Monthly Reviews](/images/output_29_1.png)

---

## Key Takeaways

- **Scalability**: PySpark handles datasets that would overwhelm other tools, making it excellent for large-scale analysis.
- **Data Exploration**: The DataFrame API simplifies summarizing, filtering, and grouping operations.
- **Verified Purchases**: Over 92% of reviews were verified purchases, showcasing strong customer trust and dataset reliability.

---

## What’s Next?

This session laid a solid foundation, but there’s more to explore. Next, I plan to share PySpark’s MLlib for predictive modeling and streaming capabilities. Enhancing visualizations and integrating PySpark into broader analytics pipelines are also on my radar.

---
