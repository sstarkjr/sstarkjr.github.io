---
title: 'Extracting Insights from Reviews: A Step-by-Step Data Processing Workflow'
date: 2025-05-01
permalink: /posts/2025/05/blog-post-2/
tags:
  - text embedding
  - product reviews
  - LLM
  - UMAP
  - data visualization
---

**Who this is for:** This post is written for data scientists, analysts, and technical practitioners who are interested in applying language models and vector embeddings to real-world unstructured data, such as customer reviews.

# Introduction

Understanding customer feedback is a classic analytics challenge. The qualitative side is just as important as the quantitative ‚Äî but much harder to process at scale. Product reviews are rich with insight, serving a range of stakeholders: *consumers* looking to make decisions, *product designers* hunting for feedback, and *marketers* seeking to better understand how people talk about the product.

But at some point, the sheer volume becomes overwhelming. How do you programmatically analyze thousands of reviews?

**Dataset:** I used a publicly available Amazon reviews dataset for the **Cuisinart SS-10P1 Single Serve Coffee Maker**. This dataset includes 2,500 reviews for this specific product.

**Process Overview:** Each review first gets broken down into a set of **key phrases** using gpt-4o-mini. I use OpenAI's text-embedding-3-large model to generate vector embeddings for those key phrases. I then apply dimensionality reduction using UMAP to project those high-dimensional vectors into a 2D space. This allows for visual exploration of semantic groupings ‚Äî helping uncover patterns without manually labeling or clustering.

**Tools used**: Python, OpenAI API, UMAP, Plotly

---

# Step 1: Gather and Clean the Review Data

The dataset comes from the Amazon open review collection. I filtered it down to one product using the ASIN: `[B014W1C2VM]`. For cost reasons, I randomly sampled 2,500 reviews.

<img src="/images/amazon_coffee_brewer.jpg" alt="Product Photo" width="400" style="display: block; margin: 1em auto;" />

Modern LLMs are forgiving of messy text, so the cleaning process was light:
- Removed duplicate entries
- Dropped null values

---

# Step 2: Extract Key Phrases with an LLM

It is important to identify key phrases because reviews often cover multiple topics and sentiments. Separating reviews into key phrase subparts enables us to untangle the review. To extract key phrases, I used a batch LLM pipeline. Here‚Äôs the core of the prompt I passed:

> *‚ÄúAnalyze the following product review. Analyze each text string, identifying key phrases that capture the most salient themes and topics.‚Äù*  
> You will be provided with product review details, and you will output a JSON object:

```json
{
  "key_phrases": [ ... ],
  "summary": "..."
}
````

> **Review Details:**
>
> * Review Title: {review\_title}
> * Review Text: {review\_text}
> * Star Rating: {rating}

> **Note:**
>
> * Rely only on the product review provided.
> * Always answer honestly and truthfully.

### ‚ö° Batch Processing Matters

Processing 2,500 reviews one-by-one at \~7 seconds each would have taken over 4.5 hours. With batching, the same task finished in just \~30 minutes ‚Äî a **90% speedup**.

I like to think of batching like using multiple washing machines. One load at a time is slow; many running in parallel finishes the job quickly.

After parsing the results, I exploded each list of key phrases into individual rows in a DataFrame.

The exploded dataset went from 2,500 rows to 12,444, or an average of 5 key phrases per review.

```python
exploded_data = []

for res in results:
    try:
        custom_id_suffix = res['custom_id'].split('-')[-1]
        response_content = res['response']['body']['choices'][0]['message']['content']
        
        # Parse the response content as JSON safely
        content_dict = json.loads(response_content)
        key_phrases = content_dict.get("key_phrases", [])
        summary = content.get('summary', []),

        for phrase in key_phrases:
            exploded_data.append({"index": custom_id_suffix, "key_phrase": phrase})
    
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError for custom_id {res['custom_id']}: {e}")
        print(response_content)

# Convert exploded data into a DataFrame
exploded_df = pd.DataFrame(exploded_data)
````

---

# Step 3: Generate Embeddings for Key Phrases

With key phrases generated, I then separate each key phrase to its own line. After removing the duplicate phrases, I generate text embeddings using OpenAI‚Äôs `text-embedding-3-large` model (500 dimensions). These embeddings capture the **semantic meaning** of each phrase as a vector in high-dimensional space.


```python
def get_embeddings_batch(texts_to_embed, model="text-embedding-3-large"):
    client = OpenAI(api_key=OPENAI_API_KEY)
    response = client.embeddings.create(
        input=[str(text) for text in texts_to_embed],
        model=model,
        dimensions=500 # large: up to 3072 dimensions
    )
    return [item.embedding for item in response.data]
````

---

# Step 4: Dimensionality Reduction with UMAP

To visualize the embeddings, I used **UMAP** (Uniform Manifold Approximation and Projection) to reduce the number of dimensions from 500 dimensions to just 2.

UMAP is especially good at preserving both local and global structure ‚Äî making it ideal for semantic data like this.

```python
from umap import UMAP

umap_model = UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_coords = umap_model.fit_transform(embedding_matrix)
```

---

# Step 5: Visualize in 2D

Using Plotly, I created an interactive scatterplot where each dot is a key phrase. You can zoom, hover, and explore how semantically similar concepts naturally cluster together.

Some examples:

* "hot water dispenser" and "fast heating" are close together
* Cleaning-related phrases form their own group
* Mentions of "programmable timer" and "auto shut-off" sit near each other

<iframe src="/images/key-phrase-embedings.html" width="1000" height="1000" frameborder="0"></iframe>

‚û°Ô∏è *Try zooming in ‚Äî many clusters are quite interpretable.*

---

# Conclusion

This workflow transforms raw text into structured, visual insights through:

* ‚úÖ **Key phrase extraction using an LLM**
* ‚úÖ **Semantic embedding with OpenAI**
* ‚úÖ **Dimensionality reduction with UMAP**
* ‚úÖ **Interactive 2D visualization with Plotly**

This approach helps uncover patterns that would be nearly impossible to detect manually.

---

### üß≠ What's Next?

In a future post, I‚Äôll explore how to:

* Cluster the key phrases
* Automatically label topic groups
* Extract actionable insights across thousands of reviews

---


### Citations

Hou, Y., Li, J., He, Z., Yan, A., Chen, X., & McAuley, J. (2024). *Bridging Language and Items for Retrieval and Recommendation*. arXiv preprint [arXiv:2403.03952](https://arxiv.org/abs/2403.03952).













---

# Step 1: Gather and Clean the Review Data

The dataset comes from the Amazon open review collection. I filtered it down to one product using the ASIN: `[B014W1C2VM]`. For cost reasons, I randomly sampled 2,500 reviews.

<img src="/images/amazon_coffee_brewer.jpg" alt="Product Photo" width="400" style="display: block; margin: 1em auto;" />

Modern LLMs are forgiving of messy text, so the cleaning process was light:

* Removed duplicate entries
* Dropped null values

---

# Step 2: Extract Key Phrases with an LLM

It is important to identify key phrases because reviews often cover multiple topics and sentiments. Separating reviews into key phrase subparts enables us to untangle the review. To extract key phrases, I used a batch LLM pipeline. Here‚Äôs the core of the prompt I passed:

> *‚ÄúAnalyze the following product review. Analyze each text string, identifying key phrases that capture the most salient themes and topics.‚Äù*
> You will be provided with product review details, and you will output a JSON object:

```json
{
  "key_phrases": [ ... ],
  "summary": "..."
}
```

> **Review Details:**
>
> * Review Title: {review\_title}
> * Review Text: {review\_text}
> * Star Rating: {rating}

> **Note:**
>
> * Rely only on the product review provided.
> * Always answer honestly and truthfully.

### ‚ö° Batch Processing Matters

Processing 2,500 reviews one-by-one at \~7 seconds each would have taken over 4.5 hours. With batching, the same task finished in just \~30 minutes ‚Äî a **90% speedup**.

I like to think of batching like using multiple washing machines. One load at a time is slow; many running in parallel finishes the job quickly.

After parsing the results, I exploded each list of key phrases into individual rows in a DataFrame.

The exploded dataset went from 2,500 rows to 12,444, or an average of 5 key phrases per review.

```python
exploded_data = []

for res in results:
    try:
        custom_id_suffix = res['custom_id'].split('-')[-1]
        response_content = res['response']['body']['choices'][0]['message']['content']
        
        # Parse the response content as JSON safely
        content_dict = json.loads(response_content)
        key_phrases = content_dict.get("key_phrases", [])
        summary = content.get('summary', []),

        for phrase in key_phrases:
            exploded_data.append({"index": custom_id_suffix, "key_phrase": phrase})
    
    except json.JSONDecodeError as e:
        print(f"JSONDecodeError for custom_id {res['custom_id']}: {e}")
        print(response_content)

# Convert exploded data into a DataFrame
exploded_df = pd.DataFrame(exploded_data)
```

---

# Step 3: Generate Embeddings for Key Phrases

With key phrases generated, I then separate each key phrase to its own line. After removing the duplicate phrases, I generate text embeddings using OpenAI‚Äôs `text-embedding-3-large` model (500 dimensions). These embeddings capture the **semantic meaning** of each phrase as a vector in high-dimensional space.

```python
def get_embeddings_batch(texts_to_embed, model="text-embedding-3-large"):
    client = OpenAI(api_key=OPENAI_API_KEY)
    response = client.embeddings.create(
        input=[str(text) for text in texts_to_embed],
        model=model,
        dimensions=500 # large: up to 3072 dimensions
    )
    return [item.embedding for item in response.data]
```

---

# Step 4: Dimensionality Reduction with UMAP

To visualize the embeddings, I used **UMAP** (Uniform Manifold Approximation and Projection) to reduce the number of dimensions from 500 dimensions to just 2.

UMAP is especially good at preserving both local and global structure ‚Äî making it ideal for semantic data like this.

```python
from umap import UMAP

umap_model = UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_coords = umap_model.fit_transform(embedding_matrix)
```

---

# Step 5: Visualize in 2D

Using Plotly, I created an interactive scatterplot where each dot is a key phrase. You can zoom, hover, and explore how semantically similar concepts naturally appear near one another.

Some examples:

* "hot water dispenser" and "fast heating" are close together
* Cleaning-related phrases form their own region
* Mentions of "programmable timer" and "auto shut-off" sit near each other

<iframe src="/images/key-phrase-embedings.html" width="1000" height="1000" frameborder="0"></iframe>

‚û°Ô∏è *Try zooming in ‚Äî semantically similar phrases tend to appear near each other, revealing natural groupings.*

---

# Conclusion

This workflow transforms raw text into structured, visual insights through:

* ‚úÖ **Key phrase extraction using an LLM**
* ‚úÖ **Semantic embedding with OpenAI**
* ‚úÖ **Dimensionality reduction with UMAP**
* ‚úÖ **Interactive 2D visualization with Plotly**

This approach enables a visual understanding of the key ideas and themes embedded across thousands of product reviews ‚Äî all without needing to manually label or categorize anything.

---

### Final Thoughts

Combining LLMs, embeddings, and dimensionality reduction unlocks structure in unstructured text. Even without clustering or summarization, visualizing the embeddings can reveal patterns, redundancies, and themes that guide more informed product, marketing, or UX decisions.

---

### Citations

Hou, Y., Li, J., He, Z., Yan, A., Chen, X., & McAuley, J. (2024). *Bridging Language and Items for Retrieval and Recommendation*. arXiv preprint [arXiv:2403.03952](https://arxiv.org/abs/2403.03952).
