---
title: 'Extracting Insights from Reviews: A Step-by-Step Data Processing Workflow
Introduction'
date: 2025-05-01
permalink: /posts/2025/10/blog-post-2/
tags:
  - text embedding
  - category1
---

# Introduction
Understanding customer feedback is a classic analytics challenge. The qualitative side is just as important as the quantitative—but much harder to process at scale. Product reviews are rich with insight, serving a range of stakeholders: consumers looking to make decisions, product designers hunting for feedback, and marketers seeking to better understand how people talk about the product.

But at some point, the sheer volume becomes overwhelming. How do you programmatically analyze thousands of reviews?

To tackle this, I break down each review into a set of key phrases using a large language model. In my prompt, I define a key phrase as a short, standalone snippet that captures one salient idea, theme, or sentiment. In practice, this has worked extremely well to tease out the multiple distinct ideas often embedded in a single review.

For this example, I chose a publicly available dataset of Amazon reviews for the Cuisinart SS-10P1 Single Serve Coffee Maker. It’s a product I know well—and with thousands of reviews and many features, it’s ideal for surfacing rich customer feedback.

Tools used: Python, OpenAI API, UMAP, Plotly


# Step 1: Gather and Clean the Review Data
The dataset comes from the Amazon open review collection. I filtered down to just the reviews for one product using the ASIN: [INSERT_ASIN_HERE].

Modern LLMs require minimal text cleaning. The main steps I included:
- de-duplicated data
- removed nulls

➡️ [Insert code block or diagram of your initial dataframe structure]

# Step 2: Extract Key Phrases with an LLM
Use of batch API to extract key concepts from each review

[Example Prompt]

[Use Batch Processing to Crate Tasks]
Batch processing is significantly faster and more efficient that non-batch. To conceptualize this, I always think of the classic laundry machine problem. You could have multiple loads of laundry to do. You have to wait for each load to finish before starting the next load. With batch predicitons, you can separate the loads into tasks, and send them off where there are multiple washing machines. Now, each load is able to run in parallel and you have saved a tremendous amount of time. 

In this example, I had 2,500 reviews to process. Had I processed them one at a time using list comprehension or similar, at 7 seconds per iteration, it would've taken about 17,500 seconds. However, batch predictions was able to complete the process in 1,800 seconds, or 10% of the time. 

[Output format and how you parse it]

# Step 3: Generate Embeddings for Key Phrases
Embedding model used (e.g., OpenAI text-embedding-3-small, HuggingFace, etc.)

With the key phrases separated to their own lines, the next step is to generate text embeddings. Text embedding models convert text into vector representations. I chose to use [insert large llm model 3] with a dimensionality of 500.

# 5. Step 4: Dimensionality Reduction with UMAP
I'd like to visualize these vectors on a plot. It's not possible to see 500 dimensions, so we need a dimensionality reduction method to reduce down to two dimentions.

For this, I chose to use UMAP.

# 6. Step 5: Visualize in 2D
Plot using Plotly for interactivity (hover over key phrases, cluster coloring, etc.)

Voila! Look at those vectors. Try zooming in, there are natural clusters of semantically similar text that group together nicely. This wouldn't have been possible with the original texts. The texts often have multiple sentiments and statements and this is simply too much noise for an informal clustering exercise.

With these clusters, you can start to see natural topics and phrases that are mentioned in multiple product reviews. This is the basis for extracting structured data from unstructured texts.
<!-- Optional: label dense clusters manually or with a tool -->

7. Conclusion
Summary of the pipeline

What kinds of insights this unlocks

Link to GitHub repo or notebook (if available)

What’s next: e.g., clustering and labeling (tease future post)



Below is a visualization of "key phrases" generated by a Large Language Model. These key phrases are integral in the beginning of a topic analysis exercise. Unstructured product reviews are rich in user commentary and simple review analysis doesn't provide meaningful or actionable insights. By visualizing key phrases you can start to see patterns in the things that people are talking about. I'll save the clustering approac for another blog post. For now, feel free to explore the interactive visual below -- try zooming in and out or hovering over the individual points.

<iframe src="/images/key-phrase-embedings.html" width="1000" height="1000" frameborder="0"></iframe>

The problem with unstructured review text is that it can be difficult to analyze in a systematic way. Traditional quantitative analysis tells only half the story (arguably less...). This challenge is complicated by the size of most modern day review datasets. Reviews often cover multiple topics (both positive and negative) in a single review. Therefore it is important to be as granular as possible for parsing these separate topics and to do it in a scalable way that handles the size of a large reviews dataset.

I started with an Amazon reviews dataset provided by []. I filtered down to reviews for only a single product, the Cuisinart Coffee Maker, Single Serve 72-Ounce Reservoir Coffee Machine, Programmable Brewing & Hot Water Dispenser, Stainless Steel, SS-10P1,Silver. I sampled down to 2,000 random unique reviews due to OpenAI cost limitations. With a larger budget, this is not required.

#TODO insert quant measure of reviews

With that done, I turned to OpenAI to help generate key phrases. I took inspiration for this approach from this article [reference article]

I used this prompt.

Here is a screenshot of a couple reviews demonstrating how it works.

I then took these key phrases and expanded the dataframe to separate rows and generated text embeddings for each review. I then used UMAP projection for dimensionality reduction. I did this so I can generate a graphic representation of the text embeddings.

Here is the visual. Note, this is interactive. A few interesting clusters that are obvious that I want to explore further in a follow-up clustering analysis.
- Error code
- general product praise
- Comparisons to Keurig
- Lighting issues

# Conclusion
Summarize key takeaways: How each step contributes to meaningful insights.
Encourage readers to experiment with similar workflows and stay tuned for the clustering post.
Technical Tips and Insights
Add any lessons learned or optimizations you made during this project (e.g., handling LLM API rate limits, embedding size considerations).


1. Aggregate & filter data as needed
2. Generate key-phrases from the reviews using batch prediction LLM call
3. Parse the results. Add to datafrae. Expand each key-phrase to be it's own row. 
4. Generate text embeddings.
5. Dimensionality Reduction using UMAP.
6. Visual Confirmation of patterns
7. Clustering Methods (next post)

# Citations
@article{hou2024bridging,
  title={Bridging Language and Items for Retrieval and Recommendation},
  author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2403.03952},
  year={2024}
}